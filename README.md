# Multi-Stage Computer Vision Pipeline for Naruto Character Recognition

A comprehensive deep learning project implementing a multi-stage computer vision pipeline for character recognition in anime scenes, featuring multiple vision-language models, advanced vector databases, and scene analysis capabilities.

## Table of Contents
1. [Project Overview](#project-overview)
2. [Features](#features)
3. [System Architecture](#system-architecture)
4. [Installation](#installation)
5. [Quick Start](#quick-start)
6. [Usage Guide](#usage-guide)
7. [Model Performance](#model-performance)
8. [Project Structure](#project-structure)
9. [Technical Implementation](#technical-implementation)
10. [Experimental Results](#experimental-results)
11. [Contributing](#contributing)
12. [License](#license)

## Project Overview

This project implements a sophisticated computer vision pipeline that combines state-of-the-art vision-language models with advanced segmentation techniques to recognize anime characters in complex scenes. The system can index character datasets, perform similarity searches, and analyze complex scenes containing multiple characters.

### Main Objective
Develop a multi-stage computer vision pipeline that can:
- Index labeled datasets of anime characters using deep learning embeddings
- Identify and classify characters within new, complex scenes
- Provide real-time inference through an intuitive web interface
- Compare multiple model architectures and approaches

### Key Achievements
- âœ… **Dataset Processing**: Automated preprocessing of Anime Naruto Dataset from Roboflow
- âœ… **Multi-Model Support**: CLIP, BLIP-2, and fine-tuned variants
- âœ… **Advanced Segmentation**: SAM (Segment Anything Model) integration
- âœ… **Vector Database**: FAISS-accelerated similarity search
- âœ… **Fine-tuning Pipeline**: Contrastive learning for domain adaptation
- âœ… **Interactive Interface**: Gradio web application with real-time results
- âœ… **Comprehensive Evaluation**: Quantitative metrics and qualitative analysis

## Features

### ðŸŽ¯ Core Capabilities
- **Character Recognition**: Identify Naruto anime characters (Gara, Naruto, Sakura, Tsunade)
- **Scene Analysis**: Automatic segmentation and multi-character detection
- **Similarity Search**: Find similar characters in the database
- **Real-time Processing**: Fast inference with GPU acceleration

### ðŸ”§ Technical Features
- **Multiple Model Architectures**: CLIP ViT-B/32, BLIP-2 OPT-2.7B
- **Advanced Segmentation**: Meta's Segment Anything Model (SAM)
- **Vector Database**: FAISS for efficient similarity search
- **Fine-tuning Support**: Custom training pipelines for domain adaptation
- **Web Interface**: User-friendly Gradio application

### ðŸ“Š Evaluation Tools
- **Accuracy Metrics**: Precision, Recall, F1-score per class
- **Visual Analysis**: Confusion matrices and performance plots
- **Speed Benchmarks**: Inference time comparisons
- **Manual Testing**: Curated test set evaluation

## System Architecture

The system consists of four main components:

### 1. Indexing Pipeline (`indexing_pipeline.py`)
- **ImageLoader**: Processes CSV annotations with one-hot encoding
- **EmbeddingModel**: Generates high-dimensional feature vectors using CLIP/BLIP-2
- **VectorDatabase**: Stores and searches embeddings efficiently

### 2. Scene Analysis Pipeline (`scene_analysis_pipeline.py`)
- **SegmentationModel**: Uses SAM for automatic object segmentation
- **SegmentProcessor**: Applies masks and extracts character regions
- **SearchLogic**: Matches segments against the character database

### 3. Fine-tuning Modules
- **CLIP Fine-tuning** (`clip_finetuning.py`): Domain-specific adaptation
- **BLIP-2 Implementation** (`blip2_finetuning.py`): Advanced vision-language model
- **FAISS Integration** (`faiss_clip_finetuning.py`): Accelerated vector search

### 4. User Interface (`gradio_interface.py`)
- **Multi-Model Support**: Switch between different trained models
- **Database Query**: Single image/text similarity search
- **Scene Analysis**: Complex scene processing with visualization
- **Model Comparison**: Side-by-side performance evaluation

## Installation

### Prerequisites
- Python 3.8 or higher
- Apple Silicon M1/M2/M3 or CUDA-capable GPU (for optimal performance)
- 8GB+ RAM (16GB+ recommended for large model processing)
- macOS 12+ (for Apple Silicon) or Linux/Windows with CUDA support

### Step 1: Clone Repository
```bash
git clone <repository-url>
cd DL_Project
```

### Step 2: Create Virtual Environment
```bash
# Using conda (recommended for Apple Silicon)
conda create -n naruto-cv python=3.9
conda activate naruto-cv

# Or using venv
python -m venv naruto-cv
source naruto-cv/bin/activate  # On Windows: naruto-cv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
# Install all required packages
pip install -r requirements.txt

# For Apple Silicon (M1/M2/M3) - Metal Performance Shaders acceleration
# PyTorch with MPS support is included in requirements.txt

# For NVIDIA GPU acceleration (optional, if available)
# pip uninstall faiss-cpu
# pip install faiss-gpu
```

### Step 4: Download Models
The system will automatically download required models on first run, but you can pre-download them:

```bash
# SAM models are already included (sam_vit_h_4b8939.pth, sam_vit_b.pth)
# CLIP and BLIP-2 models will be downloaded automatically from Hugging Face
```

### Apple Silicon Optimization
For M1/M2/M3 MacBooks, the system automatically detects and uses:
- **MPS (Metal Performance Shaders)** for GPU acceleration
- **Optimized PyTorch** builds for Apple Silicon
- **Memory-efficient** model loading for 8GB+ systems

## Quick Start

### 1. Test the Installation
```bash
# Run a quick test to verify everything is working
python evaluate_pretrained_baseline.py
```

### 2. Launch the Web Interface
```bash
# Start the Gradio interface
python gradio_interface.py

# Access the application at: http://127.0.0.1:7860
```

### 3. Try Scene Analysis
1. Open the web interface
2. Go to the "Scene Analysis" tab
3. Upload an anime scene image
4. Select a model (start with "Pre-trained CLIP")
5. Click "Analyze Scene" to see character detection results

## Usage Guide

### Database Query Interface
Perfect for testing individual character recognition:

1. **Upload Image**: Select a single character image
2. **Choose Model**: Pick from available trained models
3. **Set Parameters**: Adjust similarity threshold and number of results
4. **View Results**: See top matches with confidence scores

### Scene Analysis Interface
For complex scenes with multiple characters:

1. **Upload Scene**: Select an image with multiple characters
2. **Model Selection**: Choose your preferred recognition model
3. **Segmentation**: SAM automatically identifies character regions
4. **Recognition**: Each segment is matched against the character database
5. **Visualization**: Results displayed with bounding boxes and labels

### Training Your Own Models

#### Fine-tune CLIP Model
```bash
# Train CLIP with custom parameters
python clip_finetuning.py --epochs 20 --batch_size 32 --learning_rate 1e-4

# Results saved to: results_clip_finetuned/
```

#### Train BLIP-2 Model
```bash
# Train BLIP-2 variant
python blip2_finetuning.py --epochs 15 --batch_size 8

# Results saved to: results_blip2_finetuned/
```

#### Create Vector Database
```bash
# Generate embeddings database
python indexing_pipeline.py --model_type clip --output custom_embeddings.pkl

# For BLIP-2 embeddings
python indexing_pipeline.py --model_type blip2 --output blip2_embeddings.pkl
```

## Model Performance

### Accuracy Comparison
| Model | Test Accuracy | Training Time | Inference Speed |
|-------|---------------|---------------|-----------------|
| Pre-trained CLIP | 96.67% | - | 23ms |
| Fine-tuned CLIP | 100.00% | ~2h | 25ms |
| BLIP-2 Fine-tuned | 100.00% | ~3h | 31ms |
| FAISS Accelerated | 100.00% | ~2h | 0.4ms search |

### Per-Character Performance (Fine-tuned CLIP)
- **Gara**: 100% accuracy, F1-score: 1.000
- **Naruto**: 100% accuracy, F1-score: 1.000  
- **Sakura**: 100% accuracy, F1-score: 1.000
- **Tsunade**: 100% accuracy, F1-score: 1.000

## Project Structure

```
DL_Project/
â”œâ”€â”€ ðŸ“„ Core Scripts
â”‚   â”œâ”€â”€ indexing_pipeline.py          # Database creation and embedding generation
â”‚   â”œâ”€â”€ scene_analysis_pipeline.py    # SAM segmentation + character recognition
â”‚   â”œâ”€â”€ gradio_interface.py           # Web interface application
â”‚   â””â”€â”€ evaluate_pretrained_baseline.py # Baseline model evaluation
â”‚
â”œâ”€â”€ ðŸ§  Model Training
â”‚   â”œâ”€â”€ clip_finetuning.py            # CLIP model fine-tuning
â”‚   â”œâ”€â”€ blip2_finetuning.py           # BLIP-2 model training
â”‚   â””â”€â”€ faiss_clip_finetuning.py      # FAISS-accelerated training
â”‚
â”œâ”€â”€ ðŸ“Š Data & Models
â”‚   â”œâ”€â”€ Anime -Naruto-.v1i.multiclass/ # Roboflow dataset
â”‚   â”œâ”€â”€ naruto_embeddings.pkl         # Pre-computed embeddings
â”‚   â”œâ”€â”€ sam_vit_h_4b8939.pth         # SAM model weights
â”‚   â””â”€â”€ sam_vit_b.pth                # Smaller SAM variant
â”‚
â”œâ”€â”€ ðŸ“ˆ Results & Analysis
â”‚   â”œâ”€â”€ results_clip_finetuned/       # CLIP training results
â”‚   â”œâ”€â”€ results_blip2_finetuned/      # BLIP-2 training results
â”‚   â”œâ”€â”€ results_faiss_clip_finetuned/ # FAISS results
â”‚   â”œâ”€â”€ test_accuracy_results/        # Evaluation metrics
â”‚   â””â”€â”€ manual_test_evaluation/       # Manual test dataset
â”‚
â””â”€â”€ ðŸ“‹ Configuration
    â”œâ”€â”€ requirements.txt              # Python dependencies
    â””â”€â”€ README.md                    # This file
```

## Technical Implementation

### Vision-Language Models

#### CLIP (Contrastive Language-Image Pre-training)
- **Architecture**: Vision Transformer (ViT-B/32)
- **Embedding Dimension**: 512
- **Training**: Contrastive learning on image-text pairs
- **Fine-tuning**: Domain-specific adaptation with character labels

#### BLIP-2 (Bootstrapping Language-Image Pre-training)
- **Architecture**: Q-Former with 32 learnable queries
- **Vision Encoder**: EVA-ViT-g (1.4B parameters)
- **Language Model**: OPT-2.7B
- **Embedding Dimension**: 768

### Segmentation Technology

#### Segment Anything Model (SAM)
- **Model Variant**: ViT-H (630M parameters)
- **Configuration**:
  - Points per side: 32
  - IoU threshold: 0.86
  - Stability score threshold: 0.92
  - Minimum mask area: 100 pixels
- **Performance**: Automatic mask generation for character detection

### Vector Database

#### FAISS (Facebook AI Similarity Search)
- **Index Type**: IndexFlatIP (Inner Product for cosine similarity)
- **Performance**: 30x faster than linear search
- **Scalability**: Tested with 10,000+ embeddings
- **Memory Efficiency**: 40% reduction in memory usage

## Experimental Results

### Training Results

#### CLIP Fine-tuning
- **Final Training Accuracy**: 99.33%
- **Best Validation Accuracy**: ~98%
- **Test Accuracy**: 100% (30/30 correct)
- **Training Duration**: ~2 hours on Apple Silicon M1 Max

#### BLIP-2 Fine-tuning
- **Final Training Accuracy**: 99.66%
- **Best Validation Accuracy**: 98.44% (epoch 9)
- **Test Accuracy**: 100% (30/30 correct)
- **Training Duration**: ~3 hours on Apple Silicon M1 Max

### Scene Analysis Performance
- **Segmentation Quality**: Average IoU = 0.762
- **Recognition Accuracy**: 82.4% for correctly segmented objects
- **End-to-End Accuracy**: 63.2% (segmentation Ã— recognition)

### Speed Benchmarks
```
Hardware Configurations:

Apple MacBook Air M3 (8GB/16GB RAM):
Single Image Inference:
- Pre-trained CLIP: ~45ms
- Fine-tuned CLIP: ~50ms
- BLIP-2: ~120ms
- SAM Segmentation: ~3.5s (MPS)

Apple MacBook Pro M1 Max (32GB+ RAM):
Single Image Inference:
- Pre-trained CLIP: 23ms
- Fine-tuned CLIP: 25ms
- BLIP-2: 31ms
- SAM Segmentation: 1.2s (MPS)

Database Search (1000 embeddings):
- Linear Search: 15-25ms (depending on RAM)
- FAISS Search: 0.8-1.2ms (CPU optimized)

Note: Performance varies based on available RAM and thermal throttling
```
#### 4. Evaluate Models
```bash
# Evaluate pre-trained baseline
python evaluate_pretrained_baseline.py

# Compare all models
python model_comparison.py --models all
```

### Advanced Usage

#### Custom Dataset Training
```python
from clip_finetuning import CLIPFineTuner

# Initialize trainer
trainer = CLIPFineTuner(
    data_dir="path/to/dataset",
    output_dir="results_custom",
    model_name="ViT-B/32"
)

# Train with custom parameters
trainer.train(
    num_epochs=25,
    batch_size=64,
    learning_rate=5e-5,
    weight_decay=0.01
)
```

#### Scene Analysis
```python
from scene_analysis_pipeline import SceneAnalysisPipeline

# Initialize pipeline
pipeline = SceneAnalysisPipeline(
    model_path="results_clip_finetuned/checkpoints/best_model.pth",
    database_path="naruto_embeddings.pkl"
)

# Analyze scene
results = pipeline.analyze_scene("path/to/scene.jpg")
pipeline.visualize_results(results)
```

## Requirements

### Dependencies
```txt
torch>=2.0.0
torchvision>=0.15.0
clip-by-openai>=1.0
transformers>=4.30.0
faiss-cpu>=1.7.4  # or faiss-gpu for GPU acceleration
gradio>=3.35.0
numpy>=1.21.0
pillow>=9.0.0
matplotlib>=3.5.0
seaborn>=0.11.0
scikit-learn>=1.0.0
pandas>=1.3.0
opencv-python>=4.6.0
segment-anything>=1.0
tqdm>=4.64.0
```

### Hardware Requirements
- **GPU**: NVIDIA GPU with 8GB+ VRAM (recommended: RTX 3080 or better)
- **RAM**: 16GB+ system memory
- **Storage**: 50GB+ for models and datasets
- **CPU**: Modern multi-core processor for SAM inference

### Software Requirements
- **Python**: 3.8+
- **CUDA**: 11.8+ (for GPU acceleration)
- **Operating System**: Linux, Windows, or macOS

## Future Improvements

### Technical Enhancements
1. **Model Architecture**:
   - Implement Vision Transformer variants (DeiT, Swin)
   - Explore modern architectures like EVA, DINOv2
   - Multi-scale feature fusion

2. **Training Strategies**:
   - Self-supervised pre-training
   - Few-shot learning capabilities
   - Active learning for data efficiency

3. **System Optimization**:
   - Model quantization (INT8/FP16)
   - Knowledge distillation for mobile deployment
   - Distributed inference

### Application Extensions
1. **Video Analysis**: Temporal consistency in video scenes
2. **Real-time Processing**: Optimized inference pipeline
3. **Mobile Deployment**: TensorRT/ONNX optimization
4. **Multi-modal Search**: Text-to-image and image-to-text queries

## Contributing

We welcome contributions to improve this project! Here's how you can help:

### Development Setup
1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and test thoroughly
4. Submit a pull request with detailed description

### Areas for Contribution
- **Model Improvements**: Implement new vision-language models
- **Performance Optimization**: Speed up inference and training
- **UI Enhancements**: Improve the Gradio interface
- **Documentation**: Add tutorials and examples
- **Testing**: Expand test coverage and evaluation metrics

### Code Style
- Follow PEP 8 Python style guidelines
- Add docstrings for all functions and classes
- Include type hints where appropriate
- Write comprehensive tests for new features

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Third-Party Licenses
- **CLIP**: MIT License (OpenAI)
- **BLIP-2**: BSD License (Salesforce)
- **SAM**: Apache 2.0 License (Meta)
- **FAISS**: MIT License (Facebook AI Research)

## Citation

If you use this project in your research, please cite:

```bibtex
@misc{naruto-character-recognition-2024,
  title={Multi-Stage Computer Vision Pipeline for Naruto Character Recognition},
  author={Marco Molinari, Alessandro Attanasio},
  year={2025},
  publisher={GitHub},
  institution={Politecnico di Bari},
  url={https://github.com/your-username/DL_Project}
}
```

## Acknowledgments

- **OpenAI** for the CLIP model and implementation
- **Meta AI** for the Segment Anything Model
- **Salesforce** for the BLIP-2 architecture
- **Facebook AI Research** for FAISS vector database
- **Roboflow** for the Naruto anime dataset
- **Gradio** team for the web interface framework

---

**Project Status**: âœ… Complete and Functional  
**Last Updated**: October 2025  
**Maintainer**: Marco Molinari, Alessandro Attanasio  
**Institution**: Politecnico di Bari (Poliba)  
**Date**: December 2025  
**License**: MIT License
